{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJCMTixS_Gl8"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eE1sgXWK51j",
        "outputId": "9a4bc3dc-9d36-4b85-dbb2-6fd32d028290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H45mvb-ONK4",
        "outputId": "d64281da-4ff4-42fa-d334-2733aa471248"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LlRMjpqwpjE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/DSAI Phase 2/Natural Language Processing/topical_chat.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "D6cliv2myFuR",
        "outputId": "12d891e4-2a85-4c9e-b38b-a61b681cf04d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-21a329a8-22ca-4492-aaa1-30e943b02cdb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>message</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fan of Google or Microsoft?</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Both are excellent technology they are helpfu...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Google provides online related services and p...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Yeah, their services are good. I'm just not a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188373</th>\n",
              "      <td>8628</td>\n",
              "      <td>Wow, it does not seem like that long. Since I...</td>\n",
              "      <td>Surprised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188374</th>\n",
              "      <td>8628</td>\n",
              "      <td>I havent seen that episode, I might google it...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188375</th>\n",
              "      <td>8628</td>\n",
              "      <td>I don't think I have either. That's an insane...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188376</th>\n",
              "      <td>8628</td>\n",
              "      <td>I did, my little brother used to love Thomas ...</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188377</th>\n",
              "      <td>8628</td>\n",
              "      <td>It did. Ringo Starr, George Carlin, and Alec ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188378 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21a329a8-22ca-4492-aaa1-30e943b02cdb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21a329a8-22ca-4492-aaa1-30e943b02cdb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21a329a8-22ca-4492-aaa1-30e943b02cdb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4c5783d4-a737-43a6-b2b3-ce8e22375457\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c5783d4-a737-43a6-b2b3-ce8e22375457')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4c5783d4-a737-43a6-b2b3-ce8e22375457 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5d9dfcb5-e409-4485-b4ee-a17fbc5ec1b1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5d9dfcb5-e409-4485-b4ee-a17fbc5ec1b1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        conversation_id                                            message  \\\n",
              "0                     1              Are you a fan of Google or Microsoft?   \n",
              "1                     1   Both are excellent technology they are helpfu...   \n",
              "2                     1   I'm not  a huge fan of Google, but I use it a...   \n",
              "3                     1   Google provides online related services and p...   \n",
              "4                     1   Yeah, their services are good. I'm just not a...   \n",
              "...                 ...                                                ...   \n",
              "188373             8628   Wow, it does not seem like that long. Since I...   \n",
              "188374             8628   I havent seen that episode, I might google it...   \n",
              "188375             8628   I don't think I have either. That's an insane...   \n",
              "188376             8628   I did, my little brother used to love Thomas ...   \n",
              "188377             8628   It did. Ringo Starr, George Carlin, and Alec ...   \n",
              "\n",
              "                      sentiment  \n",
              "0        Curious to dive deeper  \n",
              "1        Curious to dive deeper  \n",
              "2        Curious to dive deeper  \n",
              "3        Curious to dive deeper  \n",
              "4        Curious to dive deeper  \n",
              "...                         ...  \n",
              "188373                Surprised  \n",
              "188374   Curious to dive deeper  \n",
              "188375   Curious to dive deeper  \n",
              "188376                    Happy  \n",
              "188377                  Neutral  \n",
              "\n",
              "[188378 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THNKp1Ybg1Eo",
        "outputId": "25c9c66f-f26d-429f-95d2-59603c9d08a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation 1:\n",
            "Question:  Are you a fan of Google or Microsoft?\n",
            "Answer:  Both are excellent technology they are helpful in many ways. For the security purpose both are super.\n",
            "\n",
            "Conversation 2:\n",
            "Question:  I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense. \n",
            "Answer:  Google provides online related services and products, which includes online ads, search engine and cloud computing.\n",
            "\n",
            "Conversation 3:\n",
            "Question:  Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives. \n",
            "Answer:  Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.\n",
            "\n",
            "Conversation 4:\n",
            "Question:  Did you know Google had hundreds of live goats to cut the grass in the past? \n",
            "Answer:  It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015. \n",
            "\n",
            "Conversation 5:\n",
            "Question:  I like Google Chrome. Do you use it as well for your browser? \n",
            "Answer:  Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger.\n",
            "\n",
            "Conversation 6:\n",
            "Question:  By the way, do you like Fish? \n",
            "Answer:  Yes. They form a sister group of tourniquets- they make the sea water clean and remove the dust from it. Fish is the biggest part in the eco-system.\n",
            "\n",
            "Conversation 7:\n",
            "Question:  Did you know that a seahorse is the only fish to have a neck? \n",
            "Answer:  Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings.\n",
            "\n",
            "Conversation 8:\n",
            "Question:  Interesting, they also have gills. Did you know that jellyfish are immortal? \n",
            "Answer:  Yes. Fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries.\n",
            "\n",
            "Conversation 9:\n",
            "Question:  What about cats, do you like cats? I'm a dog fan myself. \n",
            "Answer:  The cat is referred as domestic cat and wild cat. They make our world very clean from rats! \n",
            "\n",
            "Conversation 10:\n",
            "Question:  Yeah, cats can be cool, but they sure do spend a lot of their time sleeping. \n",
            "Answer:  Cats hear the sounds too faint or too high frequency human ears can hear. \n",
            "\n",
            "Conversation 11:\n",
            "Question:  do you like dance?\n",
            "Answer:  Yes  I do. Did you know Bruce Lee was a cha cha dancer?\n",
            "\n",
            "Conversation 12:\n",
            "Question:  Yes he even won a hardcore cha cha championship in 1958\n",
            "Answer:  Yeah. Did you know Tupac was a ballet dancer?\n",
            "\n",
            "Conversation 13:\n",
            "Question:  Yes and he even was in the production of the nutcracker\n",
            "Answer:  Yeah. Ballet dancer go through 4 pairs of shoes a week\n",
            "\n",
            "Conversation 14:\n",
            "Question:  Yes that is a lot of shoes and also a lot of money\n",
            "Answer:  Yeah true. Did you know babies are really good at dancing?\n",
            "\n",
            "Conversation 15:\n",
            "Question:  Yes and they smile more when they hit the beat\n",
            "Answer:  Yeah they are much smarter than we give them credit for\n",
            "\n",
            "Conversation 16:\n",
            "Question:  True Did you know Jackson had a patent on a dancing device?\n",
            "Answer:  Yes it helped him smooth out his dance moves\n",
            "\n",
            "Conversation 17:\n",
            "Question:  Nice. Do you like Shakespeare?\n",
            "Answer:  Yes I do. Do you know that he popularized many phrases\n",
            "\n",
            "Conversation 18:\n",
            "Question:  Yes like good riddance, in my heart of hearts and such\n",
            "Answer:  Yes and then he also invented names like Jessica, Olivia and Miranda\n",
            "\n",
            "Conversation 19:\n",
            "Question:  Yes. And for his works you have to use old english for it to make sense\n",
            "Answer:  Yes otherwise the rhymes and puns do not seem to work out\n",
            "\n",
            "Conversation 20:\n",
            "Question:  Yes. He lived at the same time as Pocahontas too\n",
            "Answer:  I wonder if they met how that would go from there\n",
            "\n",
            "Conversation 21:\n",
            "Question:  Hey what's up do use Google very often?I really love the company and was surprised to hear that it was founded back in 1998.\n",
            "Answer:  i think everyone must use it daily! its become ingrained in every day life\n",
            "\n",
            "Conversation 22:\n",
            "Question:  Agreed. The Google headquarters in Mountain View California is nicknamed the Google Plex.\n",
            "Answer:  thats funny. The current CEO is Sundar Pichai, i didnt know Larry Page was replaced\n",
            "\n",
            "Conversation 23:\n",
            "Question:  Oh yeah I didn't know that either. I also want to go to google Plex to see the goats who mow their lawn by eating it.\n",
            "Answer:  say what now?? they have that??\n",
            "\n",
            "Conversation 24:\n",
            "Question:  Yeah apparently lol! They do that instead of hiring people to mow!\n",
            "Answer:  thats both funny and i guess imaginative. leave it to a huge tech company to employ actual goats!\n",
            "\n",
            "Conversation 25:\n",
            "Question:  Yeah exactly I am sure they are cheaper. One thing I bet they couldn't exploit is fish. I think fish are so cool there is actually a breed of jellyfish that is immortal.\n",
            "Answer:  i had rememered hearing about that before. Immortatlity is wasted on a jellyfish haha. did you know a seahorse is the only fish that has an actual neck?\n",
            "\n",
            "Conversation 26:\n",
            "Question:  That is so funny I guess I never considered a seahorse a fish. The black swallower fish sounds a lot like a snake because it can eat pray that is so large.\n",
            "Answer:  i guess they live up to their name then!\n",
            "\n",
            "Conversation 27:\n",
            "Question:  It seems they do. I also didn't know that there was a difference with how freshwater and saltwater fish drink.\n",
            "Answer:  thats crazy. i wonder why fresh water ones only use osmosis? \n",
            "\n",
            "Conversation 28:\n",
            "Question:  Yeah and saltwater fish are lucky because they can do that and drink through their mouth's.\n",
            "Answer:  seems like fresh water fish got the short end of the stick with that one. Have you ever been to a cat cafe?\n",
            "\n",
            "Conversation 29:\n",
            "Question:  I have never been to a cat cafe no, what about you? Seems like they are popular in Japan and Taiwan.\n",
            "Answer:  no but I would love to! paying hourly to hang out with adorable cats? im in!\n",
            "\n",
            "Conversation 30:\n",
            "Question:  Yeah that would be a lot of fun. I didn't realize that cats sleep so much. Must be nice.\n",
            "Answer:  i guess thats where the phrase \"cat nap\" comes from\n",
            "\n",
            "Conversation 31:\n",
            "Question:  Hi!  do you like to dance?\n",
            "Answer:  I love to dance a lot. How about you?\n",
            "\n",
            "Conversation 32:\n",
            "Question:  I am really bad, but it is a good time.\n",
            "Answer:  Dancing is a lot of fun. Did you know that Bruce Lee was a great dancer?\n",
            "\n",
            "Conversation 33:\n",
            "Question:  I heard that, winning Cha Cha championships and everything!\n",
            "Answer:  Yes that is amazing. He won the Hong Kong cha-cha championship back in 1958 in fact.\n",
            "\n",
            "Conversation 34:\n",
            "Question:  I always just thought of him as a martial arts legend.  Now he is a dance legend of sorts too!\n",
            "Answer:  Yeah!! That is correct. He was a fantastic martial artist. Did you know that Tupac danced ballet in high school?\n",
            "\n",
            "Conversation 35:\n",
            "Question:  Yeah!  He was the mouse king in the Nutcracker.  Thats pretty cool, I would definitely never have guessed that about him.\n",
            "Answer:  Neither did I. That is insane because Tupac was a famous rapper. \n",
            "\n",
            "Conversation 36:\n",
            "Question:  He was indeed, his music is even in the library of congress.\n",
            "Answer:  I didn't know this thanks for sharing.\n",
            "\n",
            "Conversation 37:\n",
            "Question:  Sure thing!  Did you hear about Michael Jackson's special patent shoes?\n",
            "Answer:  No. I know that Michael Jackson was a fantastic dancer but can you tell me more about his patent shoes if you don't mind.\n",
            "\n",
            "Conversation 38:\n",
            "Question:  There was a specific device in his shoes that helped with his extreme lean in some dance moves.\n",
            "Answer:  Wow!!! That is amazing coming from such a talented singer and dancer. I couldn't even dance like that even if I dreamed of it.\n",
            "\n",
            "Conversation 39:\n",
            "Question:  Me neither. I could never be a professional dancer.\n",
            "Answer:  I heard that some professional ballet dancer can go through four pairs of shoes in a week.\n",
            "\n",
            "Conversation 40:\n",
            "Question:  That is crazy!  That can't be cheap for them.\n",
            "Answer:  No. I think its very expensive for them to be professional ballet dancers based on this.\n",
            "\n",
            "Conversation 41:\n",
            "Question:  It has been great chatting but I must go!  Gotta go get my Bruce Lee on, the martial arts part...  definitely not the dance.\n",
            "Answer:  Ha Ha!!! It was so nice chatting with you as well!! Have a nice day!!! Bye\n",
            "\n",
            "Conversation 42:\n",
            "Question:  do you like dance?\n",
            "Answer:  I love it. Did you know Bruce Lee was a dancer?\n",
            "\n",
            "Conversation 43:\n",
            "Question:  Yes he even won a cha cha championship in 1958\n",
            "Answer:  Yeah. Did you know ballet dancers use a lot of shoes?\n",
            "\n",
            "Conversation 44:\n",
            "Question:  Yes they go through 4 pairs in a single week\n",
            "Answer:  Yeah that is a lot of shoes and also a lot of money\n",
            "\n",
            "Conversation 45:\n",
            "Question:  Yeah. Babies are also really good at dancing when they hear music\n",
            "Answer:  Yes and they smile more when they hit the beat\n",
            "\n",
            "Conversation 46:\n",
            "Question:  Yeah pretty cute. Did you know Tupac was a ballet dancer?\n",
            "Answer:  Yes he was in the nutcracker as the mouse king\n",
            "\n",
            "Conversation 47:\n",
            "Question:  Yeah. DO you like SHakespeare?\n",
            "Answer:  I love his work. Did you know he popularized many terms that we use to this day?\n",
            "\n",
            "Conversation 48:\n",
            "Question:  Yes terms like, good riddance, in my heart of heart and such\n",
            "Answer:  Yes. He lives at the same time as Pocahontas did\n",
            "\n",
            "Conversation 49:\n",
            "Question:  I wonder if they had met what he would have written about her.\n",
            "Answer:  Yeah good point. He also invented some women's names\n",
            "\n",
            "Conversation 50:\n",
            "Question:  Yes among those are Olivia, Miranda and Jessica\n",
            "Answer:  Yeah. Did you know that his works have to stick to the old english language to make sense?\n",
            "\n",
            "Conversation 51:\n",
            "Question:  Yes especially with punk and rhymes, they do not work with modern language\n",
            "Answer:  Yeah. Did you know Tchaikozsky donated his skull in hopes it will be used in Shakespeare's plays?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Group messages by conversation ID\n",
        "grouped_data = data.groupby('conversation_id')['message'].apply(list)\n",
        "\n",
        "# Prepare pairs of questions and answers for each conversation\n",
        "conversations = []\n",
        "for _, messages in grouped_data.items():\n",
        "    questions = messages[::2]  # Assuming questions are at even indices\n",
        "    answers = messages[1::2]   # Assuming answers are at odd indices\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        conversations.append({'question': question, 'answer': answer})\n",
        "\n",
        "# Print the first few conversations for verification\n",
        "for i, conv in enumerate(conversations):\n",
        "    print(f\"Conversation {i + 1}:\")\n",
        "    print(f\"Question: {conv['question']}\")\n",
        "    print(f\"Answer: {conv['answer']}\\n\")\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMfvPC7ZhF8f",
        "outputId": "bc5e8961-16e4-4f5b-8860-829ebc1afe66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation 1:\n",
            "Cleaned Question: are you a fan of google or microsoft ?\n",
            "Cleaned Answer: both are excellent technology they are helpful in many ways . for the security purpose both are super .\n",
            "\n",
            "Conversation 2:\n",
            "Cleaned Question: i 'm not a huge fan of google , but i use it a lot because i have to . i think they are a monopoly in some sense .\n",
            "Cleaned Answer: google provides online related services and products , which includes online ads , search engine and cloud computing .\n",
            "\n",
            "Conversation 3:\n",
            "Cleaned Question: yeah , their services are good . i 'm just not a fan of intrusive they can be on our personal lives .\n",
            "Cleaned Answer: google is leading the alphabet subsidiary and will continue to be the umbrella company for alphabet internet interest .\n",
            "\n",
            "Conversation 4:\n",
            "Cleaned Question: did you know google had hundreds of live goats to cut the grass in the past ?\n",
            "Cleaned Answer: it is very interesting . google provide `` chrome os '' which is a light weight os . google provided a lot of hardware mainly in 2010 to 2015 .\n",
            "\n",
            "Conversation 5:\n",
            "Cleaned Question: i like google chrome . do you use it as well for your browser ?\n",
            "Cleaned Answer: yes.google is the biggest search engine and google service figure out top 100 website , including youtube and blogger .\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation 1:\n",
            "Cleaned Question: are you a fan of google or microsoft ?\n",
            "Cleaned Answer: both are excellent technology they are helpful in many ways . for the security purpose both are super .\n",
            "\n",
            "Conversation 2:\n",
            "Cleaned Question: i 'm not a huge fan of google , but i use it a lot because i have to . i think they are a monopoly in some sense .\n",
            "Cleaned Answer: google provides online related services and products , which includes online ads , search engine and cloud computing .\n",
            "\n",
            "Conversation 3:\n",
            "Cleaned Question: yeah , their services are good . i 'm just not a fan of intrusive they can be on our personal lives .\n",
            "Cleaned Answer: google is leading the alphabet subsidiary and will continue to be the umbrella company for alphabet internet interest .\n",
            "\n",
            "Conversation 4:\n",
            "Cleaned Question: did you know google had hundreds of live goats to cut the grass in the past ?\n",
            "Cleaned Answer: it is very interesting . google provide `` chrome os '' which is a light weight os . google provided a lot of hardware mainly in 2010 to 2015 .\n",
            "\n",
            "Conversation 5:\n",
            "Cleaned Question: i like google chrome . do you use it as well for your browser ?\n",
            "Cleaned Answer: yes.google is the biggest search engine and google service figure out top 100 website , including youtube and blogger .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources if not already downloaded\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Define a function to clean and preprocess the text\n",
        "def clean_and_preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Join tokens back into a cleaned sentence\n",
        "    cleaned_text = ' '.join(tokens)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean and preprocess the questions and answers in conversations\n",
        "cleaned_conversations = []\n",
        "for conv in conversations:\n",
        "    cleaned_question = clean_and_preprocess(conv['question'])\n",
        "    cleaned_answer = clean_and_preprocess(conv['answer'])\n",
        "    cleaned_conversations.append({'question': cleaned_question, 'answer': cleaned_answer})\n",
        "\n",
        "# Print the first few cleaned conversations for verification\n",
        "for i, conv in enumerate(cleaned_conversations):\n",
        "    print(f\"Conversation {i + 1}:\")\n",
        "    print(f\"Cleaned Question: {conv['question']}\")\n",
        "    print(f\"Cleaned Answer: {conv['answer']}\\n\")\n",
        "    if i >= 4:  # Print the first 5 cleaned conversations\n",
        "     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRxwwK7c7Z8r",
        "outputId": "00ccdcc4-d225-497f-cf3f-b022f6d1faeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size for questions: 27213\n",
            "Vocabulary size for answers: 28171\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Combine all cleaned questions and answers into separate lists\n",
        "questions = [conv['question'] for conv in cleaned_conversations]\n",
        "answers = [conv['answer'] for conv in cleaned_conversations]\n",
        "\n",
        "# Initialize tokenizers for questions and answers\n",
        "tokenizer_ques = Tokenizer()  # Use <OOV> for out-of-vocabulary words\n",
        "tokenizer_ans = Tokenizer()  # Use <OOV> for out-of-vocabulary words\n",
        "\n",
        "# Fit the tokenizers on the text for questions and answers\n",
        "tokenizer_ques.fit_on_texts(questions)\n",
        "tokenizer_ans.fit_on_texts(answers)\n",
        "\n",
        "# Add '<start>' and '<end>' tokens to the tokenizers' word_index for answers\n",
        "tokenizer_ans.word_index['<start>'] = len(tokenizer_ans.word_index) + 1\n",
        "tokenizer_ans.word_index['<end>'] = len(tokenizer_ans.word_index) + 2\n",
        "\n",
        "# Save the tokenizers to files\n",
        "with open('tokenizer_ques.pkl', 'wb') as tokenizer_ques_file:\n",
        "    pickle.dump(tokenizer_ques, tokenizer_ques_file)\n",
        "with open('tokenizer_ans.pkl', 'wb') as tokenizer_ans_file:\n",
        "    pickle.dump(tokenizer_ans, tokenizer_ans_file)\n",
        "\n",
        "# Convert text to sequences of word indices for questions and answers\n",
        "sequences_ques = tokenizer_ques.texts_to_sequences(questions)\n",
        "sequences_ans = tokenizer_ans.texts_to_sequences(answers)\n",
        "\n",
        "# Find the maximum sequence length for questions and answers separately\n",
        "max_seq_length_ques = max(len(seq) for seq in sequences_ques)\n",
        "max_seq_length_ans = max(len(seq) for seq in sequences_ans)\n",
        "\n",
        "# Pad sequences to make them of the same length for questions and answers\n",
        "padded_sequences_ques = pad_sequences(sequences_ques, maxlen=max_seq_length_ques, padding='post', truncating='post')\n",
        "padded_sequences_ans = pad_sequences(sequences_ans, maxlen=max_seq_length_ans, padding='post', truncating='post')\n",
        "\n",
        "# Create input-output pairs for the encoder-decoder model for questions and answers\n",
        "input_data_ques = padded_sequences_ques[:, :-1]  # Input is the question (remove the last token)\n",
        "output_data_ans = padded_sequences_ans[:, 1:]   # Output is the answer (remove the first token)\n",
        "\n",
        "# Convert input and output sequences to numpy arrays for questions and answers\n",
        "input_data_ques = np.array(input_data_ques)\n",
        "output_data_ans = np.array(output_data_ans)\n",
        "\n",
        "# Print the vocabulary size for questions and answers separately\n",
        "vocab_size_ques = len(tokenizer_ques.word_index)\n",
        "vocab_size_ans = len(tokenizer_ans.word_index)\n",
        "print(f\"Vocabulary size for questions: {vocab_size_ques}\")\n",
        "print(f\"Vocabulary size for answers: {vocab_size_ans}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUgEXsOX1n-W",
        "outputId": "dac050ea-456f-437b-8181-b7055694475b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_seq_length_ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ppqEc11pjZ_",
        "outputId": "24582a0a-2465-41df-8ff2-4a7ec228e755"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_seq_length_ans - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "715mb5po2I50"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Create a dictionary to store the preprocessed data\n",
        "preprocessed_data = {\n",
        "    'input_data': input_data_ques,\n",
        "    'output_data': output_data_ans,\n",
        "    'tokenizer_ques': tokenizer_ques,  # Use tokenizer_ques\n",
        "    'tokenizer_ans': tokenizer_ans,    # Use tokenizer_ans\n",
        "    'max_seq_length_ques': max_seq_length_ques,  # Update variable names\n",
        "    'max_seq_length_ans': max_seq_length_ans,    # Update variable names\n",
        "    'vocab_size_ques': vocab_size_ques,  # Update variable names\n",
        "    'vocab_size_ans': vocab_size_ans     # Update variable names\n",
        "}\n",
        "\n",
        "# Save the preprocessed data to a file using Pickle\n",
        "with open('preprocessed_data.pkl', 'wb') as file:\n",
        "    pickle.dump(preprocessed_data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pElg53mOJYh1",
        "outputId": "35ae11ae-f974-4119-b6fe-91cf24235749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of training data:\n",
            "Encoder input: (72939, 129)\n",
            "Decoder input: (72939, 131)\n",
            "Decoder output: (72939, 131)\n",
            "\n",
            "Shapes of validation data:\n",
            "Encoder input: (18235, 129)\n",
            "Decoder input: (18235, 131)\n",
            "Decoder output: (18235, 131)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "encoder_input_train, encoder_input_val, decoder_input_train, decoder_input_val, decoder_output_train, decoder_output_val = train_test_split(\n",
        "    input_data_ques, output_data_ans, output_data_ans, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the training and validation sets\n",
        "print(\"Shapes of training data:\")\n",
        "print(f\"Encoder input: {encoder_input_train.shape}\")\n",
        "print(f\"Decoder input: {decoder_input_train.shape}\")\n",
        "print(f\"Decoder output: {decoder_output_train.shape}\")\n",
        "\n",
        "print(\"\\nShapes of validation data:\")\n",
        "print(f\"Encoder input: {encoder_input_val.shape}\")\n",
        "print(f\"Decoder input: {decoder_input_val.shape}\")\n",
        "print(f\"Decoder output: {decoder_output_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6qoTSwadnuo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def batch_one_hot_encode(sequences, vocab_size, batch_size=8, use_embedding=True, embedding_dim=64):\n",
        "    # Initialize list for encoded sequences\n",
        "    encoded_sequences = []\n",
        "\n",
        "    if use_embedding:\n",
        "        # Use embedding layer for more efficient memory usage\n",
        "        embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "        embedded_sequences = embedding_layer(sequences)\n",
        "        return embedded_sequences\n",
        "\n",
        "    # Process smaller chunks to prevent memory overload\n",
        "    with tf.device('/CPU:0'):  # Execute on CPU if GPU memory is a concern\n",
        "        for i in range(0, len(sequences), batch_size):\n",
        "            batch = sequences[i : i + batch_size]\n",
        "            encoded_batch = tf.one_hot(batch, depth=vocab_size)\n",
        "            encoded_sequences.append(encoded_batch)\n",
        "\n",
        "        return tf.concat(encoded_sequences, axis=0)\n",
        "\n",
        "# Example usage\n",
        "# Note: Decrease vocab_size if possible or switch to embeddings\n",
        "decoder_output_train_encoded = batch_one_hot_encode(output_data_ans, vocab_size_ans, batch_size=8, use_embedding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cpY3_4qumz1",
        "outputId": "c61646b6-c440-48cc-ac3d-bee3c1941dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of your data:\n",
            "Encoder input: (91174, 130)\n",
            "Decoder input: (91174, 132)\n",
            "\n",
            "Sample 1\n",
            "Encoder Input: ['are you a fan of google or microsoft']\n",
            "Decoder Input: ['both are excellent technology they are helpful in many ways for the security purpose both are super']\n",
            "\n",
            "Sample 2\n",
            "Encoder Input: [\"i 'm not a huge fan of google but i use it a lot because i have to i think they are a monopoly in some sense\"]\n",
            "Decoder Input: ['google provides online related services and products which includes online ads search engine and cloud computing']\n",
            "\n",
            "Sample 3\n",
            "Encoder Input: [\"yeah their services are good i 'm just not a fan of intrusive they can be on our personal lives\"]\n",
            "Decoder Input: ['google is leading the alphabet subsidiary and will continue to be the umbrella company for alphabet internet interest']\n",
            "\n",
            "Sample 4\n",
            "Encoder Input: ['did you know google had hundreds of live goats to cut the grass in the past']\n",
            "Decoder Input: [\"it is very interesting google provide chrome os '' which is a light weight os google provided a lot of hardware mainly in 2010 to 2015\"]\n",
            "\n",
            "Sample 5\n",
            "Encoder Input: ['i like google chrome do you use it as well for your browser']\n",
            "Decoder Input: ['yes google is the biggest search engine and google service figure out top 100 website including youtube and blogger']\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizers\n",
        "with open('tokenizer_ques.pkl', 'rb') as tokenizer_ques_file:\n",
        "    tokenizer_ques = pickle.load(tokenizer_ques_file)\n",
        "\n",
        "with open('tokenizer_ans.pkl', 'rb') as tokenizer_ans_file:\n",
        "    tokenizer_ans = pickle.load(tokenizer_ans_file)\n",
        "\n",
        "# Load or define your sequences\n",
        "# sequences_ques = load_sequences('encoder_sequences.pkl')\n",
        "# sequences_ans = load_sequences('decoder_sequences.pkl')\n",
        "\n",
        "# Pad sequences to ensure uniform shape\n",
        "padded_sequences_ques = tf.keras.preprocessing.sequence.pad_sequences(sequences_ques, padding='post')\n",
        "padded_sequences_ans = tf.keras.preprocessing.sequence.pad_sequences(sequences_ans, padding='post')\n",
        "\n",
        "# Verify the shapes of your data\n",
        "print(\"Shapes of your data:\")\n",
        "print(\"Encoder input:\", np.array(padded_sequences_ques).shape)\n",
        "print(\"Decoder input:\", np.array(padded_sequences_ans).shape)\n",
        "\n",
        "# Sample inspection\n",
        "for i in range(5):  # Print the first 5 samples\n",
        "    print(\"\\nSample\", i + 1)\n",
        "    print(\"Encoder Input:\", tokenizer_ques.sequences_to_texts([sequences_ques[i]]))\n",
        "    print(\"Decoder Input:\", tokenizer_ans.sequences_to_texts([sequences_ans[i]]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyzUS_AfGZ4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define a custom dataset class\n",
        "class MyDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output, tknizer_ques, tknizer_ans, max_len):\n",
        "        self.encoder_input = encoder_input.tolist()\n",
        "        self.decoder_input = decoder_input.tolist()\n",
        "        self.decoder_output = decoder_output.tolist()\n",
        "        self.tknizer_ques = tknizer_ques\n",
        "        self.tknizer_ans = tknizer_ans\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_input)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        encoder_input = self.encoder_input[i]\n",
        "        decoder_input = self.decoder_input[i]\n",
        "        decoder_output = self.decoder_output[i]\n",
        "\n",
        "        encoder_seq = self.tknizer_ques.texts_to_sequences([encoder_input])[0]\n",
        "        decoder_inp_seq = self.tknizer_ans.texts_to_sequences([decoder_input])[0]\n",
        "        decoder_out_seq = self.tknizer_ans.texts_to_sequences([decoder_output])[0]\n",
        "\n",
        "        encoder_seq = pad_sequences([encoder_seq], maxlen=self.max_len, padding='post')[0]\n",
        "        decoder_inp_seq = pad_sequences([decoder_inp_seq], maxlen=self.max_len, padding='post')[0]\n",
        "        decoder_out_seq = pad_sequences([decoder_out_seq], maxlen=self.max_len, padding='post')[0]\n",
        "\n",
        "        # Return data as tf.Tensor\n",
        "        return (tf.convert_to_tensor([encoder_seq, decoder_inp_seq], dtype=tf.int32),\n",
        "                tf.convert_to_tensor(decoder_out_seq, dtype=tf.int32))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        indices = np.arange(len(self.encoder_input))\n",
        "        np.random.shuffle(indices)\n",
        "        self.encoder_input = [self.encoder_input[i] for i in indices]\n",
        "        self.decoder_input = [self.decoder_input[i] for i in indices]\n",
        "        self.decoder_output = [self.decoder_output[i] for i in indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPlUNTL4fTse"
      },
      "outputs": [],
      "source": [
        "# Define a custom data loader class\n",
        "class Dataloder(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = [self.dataset[j] for j in range(start, stop)]\n",
        "\n",
        "        encoder_seqs = np.stack([item[0][0] for item in data], axis=0)\n",
        "        decoder_inp_seqs = np.stack([item[0][1] for item in data], axis=0)\n",
        "        decoder_out_seqs = np.stack([item[1] for item in data], axis=0)\n",
        "\n",
        "        # Convert data to tf.Tensor\n",
        "        return (tf.convert_to_tensor(encoder_seqs, dtype=tf.int32),\n",
        "                tf.convert_to_tensor(decoder_inp_seqs, dtype=tf.int32)), tf.convert_to_tensor(decoder_out_seqs, dtype=tf.int32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeSP8qHCn_QL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Pad your sequences to the max sequence length\n",
        "encoder_input_train = pad_sequences(encoder_input_train, maxlen=max_seq_length_ques, padding='post')\n",
        "decoder_input_train = pad_sequences(decoder_input_train, maxlen=max_seq_length_ques, padding='post')\n",
        "decoder_output_train = pad_sequences(decoder_output_train, maxlen=max_seq_length_ques, padding='post')\n",
        "\n",
        "encoder_input_val = pad_sequences(encoder_input_val, maxlen=max_seq_length_ques, padding='post')\n",
        "decoder_input_val = pad_sequences(decoder_input_val, maxlen=max_seq_length_ques, padding='post')\n",
        "decoder_output_val = pad_sequences(decoder_output_val, maxlen=max_seq_length_ques, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGH4wtiNfYZl"
      },
      "outputs": [],
      "source": [
        "# Step 9: Define the Encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(input_dim=inp_vocab_size, output_dim=embedding_size, input_length=input_length, mask_zero=True, name=\"embedding_layer_encoder\")\n",
        "        self.lstm = LSTM(lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "    def call(self, input_sequence, training=True):\n",
        "        input_embedd = self.embedding(input_sequence)\n",
        "        lstm_output, lstm_state_h, lstm_state_c = self.lstm(input_embedd)\n",
        "        return lstm_output, lstm_state_h, lstm_state_c\n",
        "\n",
        "    def initialize_states(self, batch_size):\n",
        "        hidden_state = np.zeros((batch_size, self.lstm_size))\n",
        "        cell_state = np.zeros((batch_size, self.lstm_size))\n",
        "        return hidden_state, cell_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa6WpLC0fa_j"
      },
      "outputs": [],
      "source": [
        "# Step 10: Define the Decoder class\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, out_vocab_size, embedding_size, lstm_size, input_length):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(input_dim=out_vocab_size, output_dim=embedding_size, input_length=input_length, mask_zero=True, name=\"embedding_layer_decoder\", trainable=False)\n",
        "        self.lstm = LSTM(lstm_size, return_sequences=True, return_state=True, name=\"Decoder_LSTM\")\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        embedd = self.embedding(input_sequence)\n",
        "        lstm_output, final_state_h, final_state_c = self.lstm(embedd, initial_state=states)\n",
        "        return lstm_output, final_state_h, final_state_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3uKL9QpfeRa"
      },
      "outputs": [],
      "source": [
        "# Step 11: Define the Encoder-Decoder class\n",
        "class EncoderDecoder(tf.keras.Model):\n",
        "    def __init__(self, encoder_inputs_length, decoder_inputs_length, output_vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(inp_vocab_size=vocab_size_ques+1, embedding_size=100, lstm_size=512, input_length=encoder_inputs_length)\n",
        "        self.decoder = Decoder(out_vocab_size=vocab_size_ans+1, embedding_size=100, lstm_size=512, input_length=decoder_inputs_length)\n",
        "        self.dense = Dense(output_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, data):\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(data[0])\n",
        "        decode, _, _ = self.decoder(data[1], [encoder_h, encoder_c])\n",
        "        decoder_outputs = self.dense(decode)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfO_PtZ4fj-y"
      },
      "outputs": [],
      "source": [
        "# Step 13: Create training and validation datasets\n",
        "train_dataset = MyDataset(encoder_input_train, decoder_input_train, decoder_output_train, tokenizer_ques, tokenizer_ans, max_seq_length_ques-1)\n",
        "val_dataset = MyDataset(encoder_input_val, decoder_input_val, decoder_output_val, tokenizer_ques, tokenizer_ans, max_seq_length_ques-1)\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=32)\n",
        "val_dataloader = Dataloder(val_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBNQHqMWfreM",
        "outputId": "9fd1356e-bef6-4dea-a45c-d4160cfce036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of training data:\n",
            "Encoder input: (72939, 129)\n",
            "Decoder input: (72939, 131)\n",
            "Decoder output: (72939, 131)\n"
          ]
        }
      ],
      "source": [
        "# Step 15: Inspect training data shapes\n",
        "print(\"Shapes of training data:\")\n",
        "print(f\"Encoder input: {encoder_input_train.shape}\")\n",
        "print(f\"Decoder input: {decoder_input_train.shape}\")\n",
        "print(f\"Decoder output: {decoder_output_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXdVkJx7iqYy"
      },
      "outputs": [],
      "source": [
        "# Step 17: Define the training parameters\n",
        "epochs = 10  # Adjust based on needs\n",
        "batch_size = 32\n",
        "\n",
        "# Convert custom dataloaders to tf.data.Dataset\n",
        "def convert_to_tf_dataset(dataloader):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: iter(dataloader),\n",
        "        output_signature=(\n",
        "            (tf.TensorSpec(shape=(None, max_seq_length_ques-1), dtype=tf.int32),\n",
        "             tf.TensorSpec(shape=(None, max_seq_length_ans-1), dtype=tf.int32)),\n",
        "            tf.TensorSpec(shape=(None, max_seq_length_ans-1), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuYjTBALisRz"
      },
      "outputs": [],
      "source": [
        "# Create tf.data.Dataset from the custom dataloaders\n",
        "train_dataset = convert_to_tf_dataset(train_dataloader).batch(batch_size)\n",
        "val_dataset = convert_to_tf_dataset(val_dataloader).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpIUwsEOkg5Q",
        "outputId": "65350797-ba67-4a34-ced7-c70261d8f180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 84ms/step - accuracy: 0.9166 - loss: 0.6999 - val_accuracy: 0.9928 - val_loss: 0.0663\n",
            "Epoch 2/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 84ms/step - accuracy: 0.9941 - loss: 0.0500 - val_accuracy: 0.9966 - val_loss: 0.0320\n",
            "Epoch 3/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 84ms/step - accuracy: 0.9974 - loss: 0.0177 - val_accuracy: 0.9973 - val_loss: 0.0276\n",
            "Epoch 4/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m805s\u001b[0m 85ms/step - accuracy: 0.9985 - loss: 0.0072 - val_accuracy: 0.9975 - val_loss: 0.0278\n",
            "Epoch 5/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 84ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 0.9976 - val_loss: 0.0270\n",
            "Epoch 6/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 84ms/step - accuracy: 0.9997 - loss: 0.0010 - val_accuracy: 0.9977 - val_loss: 0.0253\n",
            "Epoch 7/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 84ms/step - accuracy: 0.9999 - loss: 4.0095e-04 - val_accuracy: 0.9978 - val_loss: 0.0229\n",
            "Epoch 8/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 1.3727e-04 - val_accuracy: 0.9979 - val_loss: 0.0220\n",
            "Epoch 9/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 4.3242e-05 - val_accuracy: 0.9980 - val_loss: 0.0206\n",
            "Epoch 10/10\n",
            "\u001b[1m9118/9118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m807s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 1.7291e-05 - val_accuracy: 0.9980 - val_loss: 0.0208\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_model(input_vocab_size, output_vocab_size, embedding_dim=64, lstm_units=256, max_seq_length=20):\n",
        "    encoder_input = layers.Input(shape=(max_seq_length,))\n",
        "    decoder_input = layers.Input(shape=(max_seq_length,))\n",
        "\n",
        "    encoder_embedded = layers.Embedding(input_vocab_size, embedding_dim)(encoder_input)\n",
        "    encoder_lstm = layers.LSTM(lstm_units, return_state=True)(encoder_embedded)\n",
        "    encoder_output, state_h, state_c = encoder_lstm\n",
        "\n",
        "    decoder_embedded = layers.Embedding(output_vocab_size, embedding_dim)(decoder_input)\n",
        "    decoder_lstm = layers.LSTM(lstm_units, return_sequences=True)(decoder_embedded, initial_state=[state_h, state_c])\n",
        "    decoder_output = layers.Dense(output_vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "    model = models.Model([encoder_input, decoder_input], decoder_output)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example dataset creation (use your dataset here)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    (encoder_input_train, decoder_input_train), decoder_output_train\n",
        "))\n",
        "train_dataset = train_dataset.batch(8)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    (encoder_input_val, decoder_input_val), decoder_output_val\n",
        "))\n",
        "val_dataset = val_dataset.batch(8)\n",
        "\n",
        "# Create the model\n",
        "input_vocab_size = len(tokenizer_ques.word_index) + 1\n",
        "output_vocab_size = len(tokenizer_ans.word_index) + 1\n",
        "model = create_model(input_vocab_size, output_vocab_size, max_seq_length=max_seq_length_ques)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsYYFEL6lGV3",
        "outputId": "5ff39855-a928-4144-beee-8c849d978e80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:61: UserWarning: Glyph 21992 (\\N{CJK UNIFIED IDEOGRAPH-55E8}) missing from current font.\n",
            "  fig.canvas.draw()\n",
            "/usr/local/lib/python3.10/dist-packages/seaborn/utils.py:648: UserWarning: Glyph 21992 (\\N{CJK UNIFIED IDEOGRAPH-55E8}) missing from current font.\n",
            "  bboxes = [l.get_window_extent() for l in labels]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming your predictions and true labels are stored as 'y_pred' and 'y_true'\n",
        "# Replace 'y_pred' and 'y_true' with the actual variables\n",
        "y_true = np.array(decoder_output_val)  # True labels from validation\n",
        "y_pred = np.array(val_dataset)  # Predicted labels from model\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=tokenizer_ans.word_index.keys(), yticklabels=tokenizer_ans.word_index.keys())\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MvMhhGo0EfWA",
        "outputId": "45d3130d-e9dd-4d0d-90b3-aa515843cfff"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_pred' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-45bff393873c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# F1 Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'F1 Score: {f1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Accuracy Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ],
      "source": [
        "# F1 Score\n",
        "f1 = f1_score(y_true.argmax(axis=1), y_pred.argmax(axis=1), average='weighted')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Accuracy Score\n",
        "accuracy = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(f'Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9t_iBsXEkfQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true.argmax(axis=1), y_pred.argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0V6862FErTJ"
      },
      "outputs": [],
      "source": [
        "# Assuming 'history' is the training history returned from model.fit()\n",
        "history = model.fit(...)\n",
        "\n",
        "# Plot Training & Validation Loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Training & Validation Accuracy\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}